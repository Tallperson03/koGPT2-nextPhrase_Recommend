{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNF3eFx1hpHQ5AH/Pwl7FQZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tallperson03/koGPT2-nextPhrase_Recommend/blob/main/koGPT2_nextPhrase_Try.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y7cK5InjzZlx",
        "outputId": "77226d07-605f-4ec1-e04b-8bd79dba30aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GVz7Q-NZ3qPt",
        "outputId": "c25910cb-fcee-44f0-fc49-ebddff3067f0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (1.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCU8lUxDPTm1",
        "outputId": "c7923e12-c0af-4d35-a286-e5d766920a80"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# KoGPT2 모델과 토크나이저 불러오기\n",
        "okt = Okt() # 형태소 분석기 객체 생성\n",
        "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained('skt/kogpt2-base-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "O3tCqn5Yz8Ag",
        "outputId": "ae3c437e-2018-4967-b554-8fbd38f6ec9d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# JSON 파일 불러오기\n",
        "with open('final_sentences.json', 'r', encoding='utf-8') as f:\n",
        "    sentence_data = json.load(f)\n",
        "\n",
        "# 학습용 텍스트 파일 생성\n",
        "with open('train.txt', 'w', encoding='utf-8') as f:\n",
        "    # 모든 카테고리를 순회하며 문장들을 파일에 쓰기\n",
        "    for category in sentence_data:\n",
        "        for sentence in category['value']:\n",
        "            f.write(sentence + '\\n')\n",
        "\n",
        "print(\"train.txt 파일 생성이 완료되었습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71sE7SzyIINq",
        "outputId": "751e7210-6439-4a59-d517-b83c3fcaafad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.txt 파일 생성이 완료되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "ciZvDc3wISoH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# 파인튜닝을 위한 데이터셋 및 데이터 콜레이터 준비\n",
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"train.txt\",\n",
        "    block_size=128\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False # Masked Language Model이 아니므로 False\n",
        ")\n",
        "\n",
        "# 학습을 위한 설정 (TrainingArguments)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/models/kogpt2-finetuned-final\", # 학습 결과물이 저장될 폴더\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3, # 전체 데이터를 3번 반복 학습\n",
        "    per_device_train_batch_size=8,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Trainer 객체 생성\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# 파인튜닝 시작\n",
        "print(\"파인튜닝을 시작합니다...\")\n",
        "trainer.train()\n",
        "print(\"파인튜닝 완료!\")\n",
        "\n",
        "# 파인튜닝된 모델과 토크나이저 저장\n",
        "trainer.save_model(\"/content/drive/MyDrive/models/kogpt2-finetuned-final\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/models/kogpt2-finetuned-final\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "51yoiuT4Iiva",
        "outputId": "8db197ce-f624-4514-ad37-9fb63f19a5ca"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "파인튜닝을 시작합니다...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [42/42 00:26, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "파인튜닝 완료!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/models/kogpt2-finetuned-final/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/models/kogpt2-finetuned-final/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/models/kogpt2-finetuned-final/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# final_words.json 파일 로드\n",
        "with open('final_words.json', 'r', encoding='utf-8') as f:\n",
        "    priority_data = json.load(f)\n",
        "\n",
        "# 모든 품사의 단어를 하나의 set으로 통합\n",
        "priority_words_set = set()\n",
        "for category in priority_data['pos_categories'].values():\n",
        "    priority_words_set.update(category['words'])\n",
        "\n",
        "print(f\"총 {len(priority_words_set)}개의 우선순위 단어를 불러왔습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVfYF-E1MOcF",
        "outputId": "e96dd52f-2deb-4ede-c4ac-9fdc6b21e0a7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 272개의 우선순위 단어를 불러왔습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_with_priority(text, num_predictions=5):\n",
        "    \"\"\"\n",
        "    (개선됨) 생성된 후보들을 '우선순위 단어 목록'에 따라 재정렬하여 반환합니다.\n",
        "    \"\"\"\n",
        "    # 1. 모델을 통해 충분한 수의 후보군 생성 (기존과 동일)\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt').to(model.device)\n",
        "    input_length = len(input_ids[0])\n",
        "\n",
        "    beam_outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=input_length + 10,\n",
        "        num_beams=num_predictions * 5,  # 더 다양한 후보를 얻기 위해 beam 개수 늘리기\n",
        "        no_repeat_ngram_size=2,\n",
        "        num_return_sequences=num_predictions * 5,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # 2. 다양성 필터를 거친 모든 후보를 일단 수집\n",
        "    all_predictions = []\n",
        "    used_keys = set()\n",
        "    for beam_output in beam_outputs:\n",
        "        # ... (기존 후처리 로직과 동일) ...\n",
        "        generated_sequence = tokenizer.decode(beam_output[input_length:], skip_special_tokens=True).strip()\n",
        "        if generated_sequence:\n",
        "            first_phrase = generated_sequence.split(' ')[0]\n",
        "            if '.' in first_phrase: first_phrase = first_phrase.split('.')[0] + '.'\n",
        "            elif '?' in first_phrase: first_phrase = first_phrase.split('?')[0] + '?'\n",
        "            elif '!' in first_phrase: first_phrase = first_phrase.split('!')[0] + '!'\n",
        "\n",
        "            nouns = okt.nouns(first_phrase)\n",
        "            if not nouns:\n",
        "                # 명사가 없는 경우 어간 추출\n",
        "                tagged = okt.pos(first_phrase, stem=True)\n",
        "                core_key = None\n",
        "                for word, pos in tagged:\n",
        "                    if pos in ['Verb', 'Adjective']:\n",
        "                        core_key = word; break\n",
        "                if not core_key: core_key = first_phrase\n",
        "            else:\n",
        "                core_key = nouns[0]\n",
        "\n",
        "            if core_key not in used_keys:\n",
        "                used_keys.add(core_key)\n",
        "                all_predictions.append(first_phrase.replace('\\n', ''))\n",
        "\n",
        "    # 3. ✨ 우선순위 단어에 따라 재정렬하는 로직 ✨\n",
        "    priority_results = []\n",
        "    other_results = []\n",
        "\n",
        "    for phrase in all_predictions:\n",
        "        # 어절의 어간/명사가 우선순위 목록에 있는지 확인\n",
        "        # (정확도를 위해 원형으로 변환 후 비교)\n",
        "        key_candidate = None\n",
        "        nouns = okt.nouns(phrase)\n",
        "        if nouns:\n",
        "            key_candidate = nouns[0]\n",
        "        else:\n",
        "            tagged = okt.pos(phrase, stem=True)\n",
        "            for word, pos in tagged:\n",
        "                if pos in ['Verb', 'Adjective', 'Noun']: # 명사도 한번 더 체크\n",
        "                    key_candidate = word\n",
        "                    break\n",
        "\n",
        "        if key_candidate in priority_words_set:\n",
        "            priority_results.append(phrase)\n",
        "        else:\n",
        "            other_results.append(phrase)\n",
        "\n",
        "    # 우선순위 결과를 먼저, 그 다음 나머지 결과를 붙여서 최종 목록 생성\n",
        "    final_sorted_list = priority_results + other_results\n",
        "\n",
        "    return final_sorted_list[:num_predictions]\n",
        "\n",
        "# --- 수정한 함수로 테스트 ---\n",
        "input_text = \"두려워하지\"\n",
        "predicted_phrases = predict_with_priority(input_text)\n",
        "print(f\"입력: '{input_text}'\")\n",
        "print(f\"추천 어절: {predicted_phrases}\")\n",
        "\n",
        "input_text2 = \"겁나\"\n",
        "predicted_phrases2 = predict_with_priority(input_text2)\n",
        "print(f\"입력: '{input_text2}'\")\n",
        "print(f\"추천 어절: {predicted_phrases2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJRGlS_Y0EI7",
        "outputId": "c2b27e43-bd13-4c41-b12a-905565b62cc8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력: '두려워하지'\n",
            "추천 어절: ['마세요.']\n",
            "입력: '겁나'\n",
            "추천 어절: ['요?', '네요.']\n"
          ]
        }
      ]
    }
  ]
}