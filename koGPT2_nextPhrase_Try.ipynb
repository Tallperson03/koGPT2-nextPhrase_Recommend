{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNF3eFx1hpHQ5AH/Pwl7FQZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tallperson03/koGPT2-nextPhrase_Recommend/blob/main/koGPT2_nextPhrase_Try.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y7cK5InjzZlx",
        "outputId": "77226d07-605f-4ec1-e04b-8bd79dba30aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GVz7Q-NZ3qPt",
        "outputId": "c25910cb-fcee-44f0-fc49-ebddff3067f0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (1.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCU8lUxDPTm1",
        "outputId": "c7923e12-c0af-4d35-a286-e5d766920a80"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# KoGPT2 ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "okt = Okt() # í˜•íƒœì†Œ ë¶„ì„ê¸° ê°ì²´ ìƒì„±\n",
        "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained('skt/kogpt2-base-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "O3tCqn5Yz8Ag",
        "outputId": "ae3c437e-2018-4967-b554-8fbd38f6ec9d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "with open('final_sentences.json', 'r', encoding='utf-8') as f:\n",
        "    sentence_data = json.load(f)\n",
        "\n",
        "# í•™ìŠµìš© í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±\n",
        "with open('train.txt', 'w', encoding='utf-8') as f:\n",
        "    # ëª¨ë“  ì¹´í…Œê³ ë¦¬ë¥¼ ìˆœíšŒí•˜ë©° ë¬¸ì¥ë“¤ì„ íŒŒì¼ì— ì“°ê¸°\n",
        "    for category in sentence_data:\n",
        "        for sentence in category['value']:\n",
        "            f.write(sentence + '\\n')\n",
        "\n",
        "print(\"train.txt íŒŒì¼ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71sE7SzyIINq",
        "outputId": "751e7210-6439-4a59-d517-b83c3fcaafad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.txt íŒŒì¼ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "ciZvDc3wISoH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# íŒŒì¸íŠœë‹ì„ ìœ„í•œ ë°ì´í„°ì…‹ ë° ë°ì´í„° ì½œë ˆì´í„° ì¤€ë¹„\n",
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"train.txt\",\n",
        "    block_size=128\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False # Masked Language Modelì´ ì•„ë‹ˆë¯€ë¡œ False\n",
        ")\n",
        "\n",
        "# í•™ìŠµì„ ìœ„í•œ ì„¤ì • (TrainingArguments)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/models/kogpt2-finetuned-final\", # í•™ìŠµ ê²°ê³¼ë¬¼ì´ ì €ì¥ë  í´ë”\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3, # ì „ì²´ ë°ì´í„°ë¥¼ 3ë²ˆ ë°˜ë³µ í•™ìŠµ\n",
        "    per_device_train_batch_size=8,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Trainer ê°ì²´ ìƒì„±\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# íŒŒì¸íŠœë‹ ì‹œì‘\n",
        "print(\"íŒŒì¸íŠœë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
        "trainer.train()\n",
        "print(\"íŒŒì¸íŠœë‹ ì™„ë£Œ!\")\n",
        "\n",
        "# íŒŒì¸íŠœë‹ëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥\n",
        "trainer.save_model(\"/content/drive/MyDrive/models/kogpt2-finetuned-final\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/models/kogpt2-finetuned-final\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "51yoiuT4Iiva",
        "outputId": "8db197ce-f624-4514-ad37-9fb63f19a5ca"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "íŒŒì¸íŠœë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [42/42 00:26, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "íŒŒì¸íŠœë‹ ì™„ë£Œ!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/models/kogpt2-finetuned-final/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/models/kogpt2-finetuned-final/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/models/kogpt2-finetuned-final/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# final_words.json íŒŒì¼ ë¡œë“œ\n",
        "with open('final_words.json', 'r', encoding='utf-8') as f:\n",
        "    priority_data = json.load(f)\n",
        "\n",
        "# ëª¨ë“  í’ˆì‚¬ì˜ ë‹¨ì–´ë¥¼ í•˜ë‚˜ì˜ setìœ¼ë¡œ í†µí•©\n",
        "priority_words_set = set()\n",
        "for category in priority_data['pos_categories'].values():\n",
        "    priority_words_set.update(category['words'])\n",
        "\n",
        "print(f\"ì´ {len(priority_words_set)}ê°œì˜ ìš°ì„ ìˆœìœ„ ë‹¨ì–´ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVfYF-E1MOcF",
        "outputId": "e96dd52f-2deb-4ede-c4ac-9fdc6b21e0a7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì´ 272ê°œì˜ ìš°ì„ ìˆœìœ„ ë‹¨ì–´ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_with_priority(text, num_predictions=5):\n",
        "    \"\"\"\n",
        "    (ê°œì„ ë¨) ìƒì„±ëœ í›„ë³´ë“¤ì„ 'ìš°ì„ ìˆœìœ„ ë‹¨ì–´ ëª©ë¡'ì— ë”°ë¼ ì¬ì •ë ¬í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    # 1. ëª¨ë¸ì„ í†µí•´ ì¶©ë¶„í•œ ìˆ˜ì˜ í›„ë³´êµ° ìƒì„± (ê¸°ì¡´ê³¼ ë™ì¼)\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt').to(model.device)\n",
        "    input_length = len(input_ids[0])\n",
        "\n",
        "    beam_outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=input_length + 10,\n",
        "        num_beams=num_predictions * 5,  # ë” ë‹¤ì–‘í•œ í›„ë³´ë¥¼ ì–»ê¸° ìœ„í•´ beam ê°œìˆ˜ ëŠ˜ë¦¬ê¸°\n",
        "        no_repeat_ngram_size=2,\n",
        "        num_return_sequences=num_predictions * 5,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # 2. ë‹¤ì–‘ì„± í•„í„°ë¥¼ ê±°ì¹œ ëª¨ë“  í›„ë³´ë¥¼ ì¼ë‹¨ ìˆ˜ì§‘\n",
        "    all_predictions = []\n",
        "    used_keys = set()\n",
        "    for beam_output in beam_outputs:\n",
        "        # ... (ê¸°ì¡´ í›„ì²˜ë¦¬ ë¡œì§ê³¼ ë™ì¼) ...\n",
        "        generated_sequence = tokenizer.decode(beam_output[input_length:], skip_special_tokens=True).strip()\n",
        "        if generated_sequence:\n",
        "            first_phrase = generated_sequence.split(' ')[0]\n",
        "            if '.' in first_phrase: first_phrase = first_phrase.split('.')[0] + '.'\n",
        "            elif '?' in first_phrase: first_phrase = first_phrase.split('?')[0] + '?'\n",
        "            elif '!' in first_phrase: first_phrase = first_phrase.split('!')[0] + '!'\n",
        "\n",
        "            nouns = okt.nouns(first_phrase)\n",
        "            if not nouns:\n",
        "                # ëª…ì‚¬ê°€ ì—†ëŠ” ê²½ìš° ì–´ê°„ ì¶”ì¶œ\n",
        "                tagged = okt.pos(first_phrase, stem=True)\n",
        "                core_key = None\n",
        "                for word, pos in tagged:\n",
        "                    if pos in ['Verb', 'Adjective']:\n",
        "                        core_key = word; break\n",
        "                if not core_key: core_key = first_phrase\n",
        "            else:\n",
        "                core_key = nouns[0]\n",
        "\n",
        "            if core_key not in used_keys:\n",
        "                used_keys.add(core_key)\n",
        "                all_predictions.append(first_phrase.replace('\\n', ''))\n",
        "\n",
        "    # 3. âœ¨ ìš°ì„ ìˆœìœ„ ë‹¨ì–´ì— ë”°ë¼ ì¬ì •ë ¬í•˜ëŠ” ë¡œì§ âœ¨\n",
        "    priority_results = []\n",
        "    other_results = []\n",
        "\n",
        "    for phrase in all_predictions:\n",
        "        # ì–´ì ˆì˜ ì–´ê°„/ëª…ì‚¬ê°€ ìš°ì„ ìˆœìœ„ ëª©ë¡ì— ìˆëŠ”ì§€ í™•ì¸\n",
        "        # (ì •í™•ë„ë¥¼ ìœ„í•´ ì›í˜•ìœ¼ë¡œ ë³€í™˜ í›„ ë¹„êµ)\n",
        "        key_candidate = None\n",
        "        nouns = okt.nouns(phrase)\n",
        "        if nouns:\n",
        "            key_candidate = nouns[0]\n",
        "        else:\n",
        "            tagged = okt.pos(phrase, stem=True)\n",
        "            for word, pos in tagged:\n",
        "                if pos in ['Verb', 'Adjective', 'Noun']: # ëª…ì‚¬ë„ í•œë²ˆ ë” ì²´í¬\n",
        "                    key_candidate = word\n",
        "                    break\n",
        "\n",
        "        if key_candidate in priority_words_set:\n",
        "            priority_results.append(phrase)\n",
        "        else:\n",
        "            other_results.append(phrase)\n",
        "\n",
        "    # ìš°ì„ ìˆœìœ„ ê²°ê³¼ë¥¼ ë¨¼ì €, ê·¸ ë‹¤ìŒ ë‚˜ë¨¸ì§€ ê²°ê³¼ë¥¼ ë¶™ì—¬ì„œ ìµœì¢… ëª©ë¡ ìƒì„±\n",
        "    final_sorted_list = priority_results + other_results\n",
        "\n",
        "    return final_sorted_list[:num_predictions]\n",
        "\n",
        "# --- ìˆ˜ì •í•œ í•¨ìˆ˜ë¡œ í…ŒìŠ¤íŠ¸ ---\n",
        "input_text = \"ë‘ë ¤ì›Œí•˜ì§€\"\n",
        "predicted_phrases = predict_with_priority(input_text)\n",
        "print(f\"ì…ë ¥: '{input_text}'\")\n",
        "print(f\"ì¶”ì²œ ì–´ì ˆ: {predicted_phrases}\")\n",
        "\n",
        "input_text2 = \"ê²ë‚˜\"\n",
        "predicted_phrases2 = predict_with_priority(input_text2)\n",
        "print(f\"ì…ë ¥: '{input_text2}'\")\n",
        "print(f\"ì¶”ì²œ ì–´ì ˆ: {predicted_phrases2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJRGlS_Y0EI7",
        "outputId": "c2b27e43-bd13-4c41-b12a-905565b62cc8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì…ë ¥: 'ë‘ë ¤ì›Œí•˜ì§€'\n",
            "ì¶”ì²œ ì–´ì ˆ: ['ë§ˆì„¸ìš”.']\n",
            "ì…ë ¥: 'ê²ë‚˜'\n",
            "ì¶”ì²œ ì–´ì ˆ: ['ìš”?', 'ë„¤ìš”.']\n"
          ]
        }
      ]
    }
  ]
}